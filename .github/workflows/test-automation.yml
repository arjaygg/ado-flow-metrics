name: Test Automation Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '20'

jobs:
  # Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: ado_flow_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libpq-dev

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock pytest-asyncio coverage

    - name: Set up environment variables
      run: |
        echo "PYTHONPATH=${{ github.workspace }}/src" >> $GITHUB_ENV
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/ado_flow_test" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_ORG=test-org" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_PROJECT=test-project" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_PAT=test-token" >> $GITHUB_ENV

    - name: Run unit tests with coverage
      run: |
        pytest tests/ -m "unit or not (integration or e2e)" \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term \
          --cov-fail-under=80 \
          --junit-xml=test-results/unit-tests.xml \
          -v

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-unit-tests

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results
        path: |
          test-results/
          htmlcov/

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: ado_flow_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libpq-dev curl

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock pytest-asyncio

    - name: Install Node.js dependencies for mock server
      run: |
        cd tests/mock-server
        npm install

    - name: Start mock Azure DevOps API server
      run: |
        cd tests/mock-server
        npm start &
        sleep 5
        # Verify mock server is running
        curl -f http://localhost:8080/health || exit 1
      env:
        PORT: 8080

    - name: Set up environment variables
      run: |
        echo "PYTHONPATH=${{ github.workspace }}/src" >> $GITHUB_ENV
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/ado_flow_test" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_BASE_URL=http://localhost:8080" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_ORG=test-org" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_PROJECT=test-project" >> $GITHUB_ENV
        echo "AZURE_DEVOPS_PAT=test-token" >> $GITHUB_ENV

    - name: Run integration tests
      run: |
        pytest tests/integration/ \
          --junit-xml=test-results/integration-tests.xml \
          -v

    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: test-results/

  # E2E Tests with Playwright
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libnss3 \
          libnspr4 \
          libatk-bridge2.0-0 \
          libdrm2 \
          libxkbcommon0 \
          libxcomposite1 \
          libxdamage1 \
          libxrandr2 \
          libgbm1 \
          libxss1 \
          libasound2

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-playwright playwright

    - name: Install Playwright browsers
      run: |
        playwright install chromium firefox webkit

    - name: Install Node.js dependencies for mock server
      run: |
        cd tests/mock-server
        npm install

    - name: Start application server
      run: |
        # Start the dashboard application
        python -m src.web_server &
        sleep 10
        # Verify server is running
        curl -f http://localhost:8050/health || curl -f http://localhost:8050/ || exit 1
      env:
        PYTHONPATH: ${{ github.workspace }}/src
        PORT: 8050

    - name: Start mock API server
      run: |
        cd tests/mock-server
        npm start &
        sleep 5
        curl -f http://localhost:8080/health || exit 1
      env:
        PORT: 8080

    - name: Run E2E tests
      run: |
        pytest tests/e2e/ \
          --junit-xml=test-results/e2e-tests.xml \
          --html=test-results/e2e-report.html \
          --self-contained-html \
          -v
      env:
        BASE_URL: http://localhost:8050
        PLAYWRIGHT_HEADLESS: "true"
        BROWSER: chromium

    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          test-results/
          playwright-report/

    - name: Upload screenshots on failure
      uses: actions/upload-artifact@v3
      if: failure()
      with:
        name: e2e-screenshots
        path: test-results/

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Run performance tests
      run: |
        pytest tests/ -m "performance" \
          --benchmark-json=test-results/benchmark.json \
          --junit-xml=test-results/performance-tests.xml \
          -v
      env:
        PYTHONPATH: ${{ github.workspace }}/src

    - name: Upload performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: test-results/

  # Docker Tests
  docker-tests:
    name: Docker Container Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build test Docker image
      run: |
        docker build -f tests/Dockerfile -t ado-flow-metrics-test .

    - name: Run tests in Docker
      run: |
        docker run --rm \
          -v ${{ github.workspace }}/test-results:/app/test-results \
          -e PYTEST_ARGS="--junit-xml=test-results/docker-tests.xml -v" \
          ado-flow-metrics-test

    - name: Upload Docker test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: docker-test-results
        path: test-results/

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security testing tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep

    - name: Run bandit security linter
      run: |
        bandit -r src/ -f json -o test-results/bandit-report.json || true
        bandit -r src/ -f txt

    - name: Run safety check for dependencies
      run: |
        safety check --json --output test-results/safety-report.json || true
        safety check

    - name: Run semgrep static analysis
      run: |
        semgrep --config=auto src/ --json --output=test-results/semgrep-report.json || true

    - name: Upload security test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: test-results/

  # Test Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, docker-tests, security-tests]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: all-test-results/

    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always()
      with:
        files: |
          all-test-results/**/*.xml
        report_individual_runs: true
        deduplicate_classes_by_file_name: false

    - name: Create test summary
      run: |
        echo "# Test Results Summary" > $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Execution Status" >> $GITHUB_STEP_SUMMARY
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- E2E Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Tests: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Docker Tests: ${{ needs.docker-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Security Tests: ${{ needs.security-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Test results and reports are available in the workflow artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Coverage reports are uploaded to Codecov" >> $GITHUB_STEP_SUMMARY

    - name: Upload combined test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: all-test-results
        path: all-test-results/
        retention-days: 30

  # Notification
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: always() && (github.event_name == 'push' || github.event_name == 'schedule')
    
    steps:
    - name: Notify on success
      if: needs.test-summary.result == 'success'
      run: |
        echo "✅ All tests passed successfully!"
        # Add notification logic here (Slack, Teams, etc.)

    - name: Notify on failure
      if: needs.test-summary.result == 'failure'
      run: |
        echo "❌ Some tests failed. Check the workflow results."
        # Add notification logic here (Slack, Teams, etc.)

# Cleanup
  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: always()
    
    steps:
    - name: Clean up test artifacts
      run: |
        echo "Cleaning up temporary test files..."
        # Add cleanup logic if needed