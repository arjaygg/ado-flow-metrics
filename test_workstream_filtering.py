#!/usr/bin/env python3
"""
Workstream Filtering Test Script

Tests the workstream filtering functionality by validating:
1. Configuration loading
2. Team member assignment logic
3. Filtering accuracy
4. JavaScript/Python equivalence
"""

import json
import os
from pathlib import Path

def load_workstream_config():
    """Load workstream configuration"""
    with open("config/workstream_config.json", "r") as f:
        return json.load(f)

def test_name_matching():
    """Test workstream name matching logic"""
    print("ğŸ” Testing Name Matching Logic...")
    
    config = load_workstream_config()
    workstreams = config["workstreams"]
    
    # Test cases with expected workstream assignments
    test_cases = [
        ("Nenissa Chen", "Data"),
        ("Ariel Santos", "Data"), 
        ("Sharon Martinez", "QA"),
        ("Apollo Rodriguez", "OutSystems"),
        ("Patrick Oniel Johnson", "Data"),  # Partial match test
        ("Kennedy Oliveira Silva", "Data"),  # Partial match test
        ("John Doe", "Others"),  # Should default to "Others"
        ("Christopher Jan", "Data"),
        ("Lorenz", "QA"),
        ("Glizzel", "OutSystems")
    ]
    
    def assign_workstream(name, workstreams_config, default="Others"):
        """Python implementation of workstream assignment logic"""
        name_lower = name.lower()
        
        for workstream_name, details in workstreams_config.items():
            patterns = details.get("name_patterns", [])
            for pattern in patterns:
                if pattern.lower() in name_lower:
                    return workstream_name
        
        return default
    
    results = []
    for name, expected in test_cases:
        actual = assign_workstream(name, workstreams)
        status = "âœ…" if actual == expected else "âŒ"
        results.append((name, expected, actual, status))
        print(f"  {status} {name} -> Expected: {expected}, Got: {actual}")
    
    # Summary
    passed = sum(1 for _, _, _, status in results if status == "âœ…")
    total = len(results)
    print(f"\n  ğŸ“Š Name matching: {passed}/{total} tests passed")
    
    return passed == total

def test_team_metrics_filtering():
    """Test filtering team metrics by workstream"""
    print("\nğŸ¯ Testing Team Metrics Filtering...")
    
    # Load test data
    with open("data/test_dashboard_data.json", "r") as f:
        test_data = json.load(f)
    
    config = load_workstream_config()
    workstreams = config["workstreams"]
    
    team_metrics = test_data["team_metrics"]
    
    def filter_team_metrics(metrics, selected_workstreams, workstreams_config):
        """Filter team metrics by selected workstreams"""
        if "All Teams" in selected_workstreams:
            return metrics
        
        filtered = {}
        for name, data in metrics.items():
            # Assign workstream to team member
            name_lower = name.lower()
            member_workstream = "Others"
            
            for workstream_name, details in workstreams_config.items():
                patterns = details.get("name_patterns", [])
                for pattern in patterns:
                    if pattern.lower() in name_lower:
                        member_workstream = workstream_name
                        break
                if member_workstream != "Others":
                    break
            
            # Include if workstream is selected
            if member_workstream in selected_workstreams:
                filtered[name] = data
        
        return filtered
    
    # Test different filter combinations
    # First, let's see actual assignments
    actual_assignments = {}
    for name in team_metrics.keys():
        name_lower = name.lower()
        member_workstream = "Others"
        
        for workstream_name, details in workstreams.items():
            patterns = details.get("name_patterns", [])
            for pattern in patterns:
                if pattern.lower() in name_lower:
                    member_workstream = workstream_name
                    break
            if member_workstream != "Others":
                break
        actual_assignments[name] = member_workstream
    
    print(f"    ğŸ” Actual workstream assignments:")
    for name, workstream in actual_assignments.items():
        print(f"      {name} -> {workstream}")
    
    # Count by workstream
    workstream_counts = {}
    for workstream in actual_assignments.values():
        workstream_counts[workstream] = workstream_counts.get(workstream, 0) + 1
    
    test_filters = [
        (["All Teams"], len(team_metrics)),  # Should include all
        (["Data"], workstream_counts.get("Data", 0)),  
        (["QA"], workstream_counts.get("QA", 0)),   
        (["OutSystems"], workstream_counts.get("OutSystems", 0)),  
        (["Data", "QA"], workstream_counts.get("Data", 0) + workstream_counts.get("QA", 0)),  
        (["NonExistent"], 0)  # Should include none
    ]
    
    results = []
    for filter_workstreams, expected_count in test_filters:
        filtered = filter_team_metrics(team_metrics, filter_workstreams, workstreams)
        actual_count = len(filtered)
        status = "âœ…" if actual_count == expected_count else "âŒ"
        results.append((filter_workstreams, expected_count, actual_count, status))
        
        print(f"  {status} Filter {filter_workstreams}: Expected {expected_count}, Got {actual_count}")
        if status == "âœ…" and actual_count > 0:
            names = list(filtered.keys())
            print(f"    Members: {', '.join(names)}")
    
    # Summary  
    passed = sum(1 for _, _, _, status in results if status == "âœ…")
    total = len(results)
    print(f"\n  ğŸ“Š Team filtering: {passed}/{total} tests passed")
    
    return passed == total

def test_javascript_config_equivalence():
    """Test that JavaScript config matches JSON config"""
    print("\nğŸ”— Testing JavaScript/JSON Config Equivalence...")
    
    # Load JSON config
    json_config = load_workstream_config()
    
    # Parse JavaScript config
    js_config_content = Path("js/workstream_config.js").read_text()
    
    # Extract the workstreams from JavaScript (simplified parsing)
    try:
        # Look for the workstreams object in the JavaScript
        start_marker = '"workstreams": {'
        end_marker = '},'
        
        start_idx = js_config_content.find(start_marker)
        if start_idx == -1:
            print("  âŒ Could not find workstreams in JavaScript config")
            return False
        
        # Find the end of the workstreams object
        bracket_count = 0
        current_idx = start_idx + len(start_marker) - 1
        end_idx = -1
        
        for i, char in enumerate(js_config_content[current_idx:], current_idx):
            if char == '{':
                bracket_count += 1
            elif char == '}':
                bracket_count -= 1
                if bracket_count == 0:
                    end_idx = i + 1
                    break
        
        if end_idx == -1:
            print("  âŒ Could not parse JavaScript workstreams object")
            return False
        
        # Basic validation - check if workstream names exist in JS
        js_content = js_config_content[start_idx:end_idx]
        
        for workstream_name in json_config["workstreams"].keys():
            if f'"{workstream_name}"' in js_content:
                print(f"  âœ… {workstream_name} found in JavaScript config")
            else:
                print(f"  âŒ {workstream_name} NOT found in JavaScript config")
                return False
        
        print("  âœ… JavaScript config structure matches JSON config")
        return True
        
    except Exception as e:
        print(f"  âŒ Error parsing JavaScript config: {e}")
        return False

def generate_workstream_report():
    """Generate a comprehensive workstream report"""
    print("\nğŸ“‹ Generating Workstream Report...")
    
    config = load_workstream_config()
    workstreams = config["workstreams"]
    
    # Load test data
    with open("data/test_dashboard_data.json", "r") as f:
        test_data = json.load(f)
    
    team_metrics = test_data["team_metrics"]
    
    report = {
        "workstream_summary": {},
        "team_assignments": {},
        "metrics_by_workstream": {}
    }
    
    # Assign each team member to workstream
    for name in team_metrics.keys():
        name_lower = name.lower()
        assigned_workstream = "Others"
        
        for workstream_name, details in workstreams.items():
            patterns = details.get("name_patterns", [])
            for pattern in patterns:
                if pattern.lower() in name_lower:
                    assigned_workstream = workstream_name
                    break
            if assigned_workstream != "Others":
                break
        
        report["team_assignments"][name] = assigned_workstream
        
        # Add to workstream summary
        if assigned_workstream not in report["workstream_summary"]:
            report["workstream_summary"][assigned_workstream] = []
        report["workstream_summary"][assigned_workstream].append(name)
    
    # Calculate metrics by workstream
    for workstream, members in report["workstream_summary"].items():
        total_completed = sum(team_metrics[member]["completed_items"] for member in members)
        total_active = sum(team_metrics[member]["active_items"] for member in members)
        avg_completion_rate = sum(team_metrics[member]["completion_rate"] for member in members) / len(members)
        avg_lead_time = sum(team_metrics[member]["average_lead_time"] for member in members) / len(members)
        
        report["metrics_by_workstream"][workstream] = {
            "member_count": len(members),
            "total_completed": total_completed,
            "total_active": total_active,
            "avg_completion_rate": round(avg_completion_rate, 1),
            "avg_lead_time": round(avg_lead_time, 1)
        }
    
    # Save report
    with open("data/workstream_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    # Display summary
    print("  ğŸ“Š Workstream Distribution:")
    for workstream, members in report["workstream_summary"].items():
        metrics = report["metrics_by_workstream"][workstream]
        print(f"    {workstream}: {len(members)} members")
        print(f"      - Completed: {metrics['total_completed']} items")
        print(f"      - Avg completion rate: {metrics['avg_completion_rate']}%")
        print(f"      - Avg lead time: {metrics['avg_lead_time']} days")
    
    print("  âœ… Report saved to: data/workstream_report.json")
    return True

def main():
    """Run workstream filtering tests"""
    print("ğŸ¯ Workstream Filtering Test Suite")
    print("=" * 50)
    
    tests = [
        ("Name Matching Logic", test_name_matching),
        ("Team Metrics Filtering", test_team_metrics_filtering), 
        ("JavaScript Config Equivalence", test_javascript_config_equivalence),
        ("Workstream Report Generation", generate_workstream_report)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"  âŒ {test_name} failed: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“‹ Workstream Filtering Test Summary:")
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status}: {test_name}")
    
    print(f"\nğŸ¯ Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("âœ… All workstream filtering tests passed!")
        return 0
    else:
        print("âŒ Some workstream filtering tests failed.")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())