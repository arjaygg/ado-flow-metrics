#!/usr/bin/env python3
"""
Demo: Live Azure DevOps to Dashboard Integration
Shows exactly how live ADO data flows to the executive dashboard
"""

import json
import os
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))


def demo_live_integration():
    """Demonstrate the complete live Azure DevOps integration process."""

    print("üîÑ LIVE AZURE DEVOPS TO DASHBOARD INTEGRATION DEMO")
    print("=" * 60)

    # Step 1: Check Environment Setup
    print("\nüìã Step 1: Environment Configuration")
    pat_token = os.getenv("AZURE_DEVOPS_PAT")
    # Get org URL from environment or config, with better error handling
    org_url = os.getenv("AZURE_DEVOPS_ORG")
    if not org_url:
        # Try to load from config file
        try:
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).parent / "src"))
            from config_manager import get_settings
            settings = get_settings()
            org_url = settings.azure_devops.org_url
            # If config contains placeholder, warn user
            if "your-org" in org_url or org_url == "https://dev.azure.com":
                org_url = None
        except:
            org_url = None
    
    if not org_url:
        org_url = "https://dev.azure.com/YOUR_ORG_HERE"  # Clear placeholder
    project = os.getenv("AZURE_DEVOPS_PROJECT", "YourProject")

    print(f"   ‚Ä¢ Organization: {org_url}")
    print(f"   ‚Ä¢ Project: {project}")
    print(
        f"   ‚Ä¢ PAT Token: {'‚úÖ Set' if pat_token and pat_token != 'your-token' else '‚ùå Not Set'}"
    )

    if not pat_token or pat_token == "your-token" or not org_url or "YOUR_ORG_HERE" in org_url or "your-org" in org_url:
        print("\n‚ö†Ô∏è  Azure DevOps configuration incomplete - will demonstrate with mock data")
        print("\nüîß To use live data, set environment variables:")
        print("   export AZURE_DEVOPS_PAT='your-personal-access-token'")
        print("   export AZURE_DEVOPS_ORG='https://dev.azure.com/YOUR_ACTUAL_ORG_NAME'")
        print("   export AZURE_DEVOPS_PROJECT='YourActualProjectName'")
        print("\nüí° Or create a proper config/config.json file from config.sample.json")
        print("   and replace placeholder values with your actual organization and project names")
        use_live_data = False
    else:
        use_live_data = True

    # Step 2: Data Extraction
    print(
        f"\nüì° Step 2: Data Extraction ({'Live ADO' if use_live_data else 'Mock Data'})"
    )

    if use_live_data:
        try:
            from azure_devops_client import AzureDevOpsClient

            print("   ‚Ä¢ Initializing Azure DevOps client...")
            client = AzureDevOpsClient(org_url, project, pat_token)

            print("   ‚Ä¢ Executing WIQL query to get work item IDs...")
            print(
                f"     SELECT [System.Id] FROM WorkItems WHERE [System.TeamProject] = '{project}'"
            )

            print("   ‚Ä¢ Fetching detailed work item information...")
            print("     GET /workitems?ids={ids}&$expand=relations")

            print("   ‚Ä¢ Retrieving state transition history...")
            print("     GET /workitems/{id}/updates")

            work_items = client.get_work_items(days_back=30)
            print(
                f"   ‚úÖ Successfully fetched {len(work_items)} work items from Azure DevOps"
            )

        except Exception as e:
            print(f"   ‚ùå Error connecting to Azure DevOps: {e}")
            print("   üìÑ Falling back to mock data...")
            use_live_data = False

    if not use_live_data:
        from mock_data import generate_mock_azure_devops_data

        print("   ‚Ä¢ Generating mock Azure DevOps data...")
        work_items = generate_mock_azure_devops_data()
        print(f"   ‚úÖ Generated {len(work_items)} mock work items")

    # Step 3: Data Transformation
    print("\nüîÑ Step 3: Data Transformation")
    print("   ‚Ä¢ Converting ADO format to flow metrics format...")

    sample_item = work_items[0] if work_items else {}
    print("   ‚Ä¢ Raw ADO Work Item Structure:")
    print(f"     ‚îú‚îÄ ID: {sample_item.get('id', 'N/A')}")
    print(f"     ‚îú‚îÄ Title: {sample_item.get('title', 'N/A')}")
    print(f"     ‚îú‚îÄ State: {sample_item.get('current_state', 'N/A')}")
    print(f"     ‚îú‚îÄ Assigned To: {sample_item.get('assigned_to', 'N/A')}")
    print(
        f"     ‚îî‚îÄ Transitions: {len(sample_item.get('state_transitions', []))} states"
    )

    # Step 4: Flow Metrics Calculation
    print("\nüìä Step 4: Flow Metrics Calculation")

    try:
        from calculator import FlowMetricsCalculator

        print("   ‚Ä¢ Initializing Flow Metrics Calculator...")
        calculator = FlowMetricsCalculator(work_items)

        print("   ‚Ä¢ Calculating core metrics:")
        print("     ‚îú‚îÄ Lead Time (Created ‚Üí Done)")
        print("     ‚îú‚îÄ Cycle Time (Active ‚Üí Done)")
        print("     ‚îú‚îÄ Throughput (Items/Period)")
        print("     ‚îú‚îÄ Work in Progress (Current WIP)")
        print("     ‚îú‚îÄ Flow Efficiency (Active/Total Time)")
        print("     ‚îî‚îÄ Team Metrics (Individual ‚Üí Aggregate)")

        report = calculator.generate_flow_metrics_report()
        print("   ‚úÖ Flow metrics calculation completed")

        # Show key metrics
        summary = report.get("summary", {})
        lead_time = report.get("lead_time", {})
        team_metrics = report.get("team_metrics", {})

        print(f"\n   üìà Key Metrics Generated:")
        print(f"     ‚Ä¢ Total Items: {summary.get('total_work_items', 0)}")
        print(f"     ‚Ä¢ Completed: {summary.get('completed_items', 0)}")
        print(f"     ‚Ä¢ Completion Rate: {summary.get('completion_rate', 0)}%")
        print(f"     ‚Ä¢ Average Lead Time: {lead_time.get('average_days', 0)} days")
        print(f"     ‚Ä¢ Team Size: {len(team_metrics)} members")

    except Exception as e:
        print(f"   ‚ùå Error calculating metrics: {e}")
        return False

    # Step 5: Executive Dashboard Output
    print("\nüíº Step 5: Executive Dashboard Integration")

    dashboard_dir = Path("../dashboard")
    output_file = dashboard_dir / "live_demo_data.json"

    try:
        # Save executive-ready JSON
        with open(output_file, "w") as f:
            json.dump(report, f, indent=2, default=str)

        print(f"   ‚úÖ Executive data saved to: {output_file}")
        print("   üìã Dashboard-ready JSON structure:")
        print("     ‚îú‚îÄ summary: Overall completion metrics")
        print("     ‚îú‚îÄ lead_time: Delivery speed analysis")
        print("     ‚îú‚îÄ cycle_time: Development efficiency")
        print("     ‚îú‚îÄ throughput: Team velocity metrics")
        print("     ‚îú‚îÄ work_in_progress: Current workload")
        print("     ‚îú‚îÄ flow_efficiency: Process optimization")
        print("     ‚îî‚îÄ team_metrics: Individual ‚Üí Team aggregation")

    except Exception as e:
        print(f"   ‚ùå Error saving dashboard data: {e}")
        return False

    # Step 6: Dashboard Instructions
    print("\nüéØ Step 6: Executive Dashboard Usage")
    print("   1. Open the executive dashboard:")
    print("      open ../dashboard/executive-index.html")
    print("   2. Click 'Upload Report' button")
    print("   3. Select 'live_demo_data.json' file")
    print("   4. View executive metrics and insights")

    # Step 7: Data Validation
    print("\n‚úÖ Step 7: Data Quality Validation")

    try:
        sys.path.insert(0, str(dashboard_dir))
        from validate_data import validate_data_structure, calculate_executive_metrics

        errors, warnings = validate_data_structure(report)

        if errors:
            print("   ‚ùå Data validation errors:")
            for error in errors:
                print(f"      ‚Ä¢ {error}")
        else:
            print("   ‚úÖ Data structure validation passed")

        if warnings:
            print("   ‚ö†Ô∏è  Data validation warnings:")
            for warning in warnings:
                print(f"      ‚Ä¢ {warning}")

        # Calculate executive insights
        exec_metrics = calculate_executive_metrics(report)
        insights = exec_metrics.get("insights", [])

        if insights:
            print("   üí° Executive Insights Generated:")
            for insight in insights:
                print(f"      ‚Ä¢ {insight}")

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Validation error: {e}")

    print("\nüéâ INTEGRATION DEMO COMPLETED SUCCESSFULLY!")
    print("\nüìã Summary:")
    print(f"   ‚Ä¢ Data Source: {'Live Azure DevOps' if use_live_data else 'Mock Data'}")
    print(f"   ‚Ä¢ Work Items Processed: {len(work_items)}")
    print(f"   ‚Ä¢ Executive JSON Generated: {output_file}")
    print(f"   ‚Ä¢ Dashboard Ready: ‚úÖ")

    return True


if __name__ == "__main__":
    success = demo_live_integration()
    sys.exit(0 if success else 1)
